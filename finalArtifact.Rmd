---
title: "finalArtifact"
date: "December 8, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, include=FALSE}
if(!require(mlbench)){install.packages("mlbench"); require(mlbench)} # common datasets to use
if(!require(tidyverse)){install.packages("tidyverse"); library(tidyverse)} 
if(!require(modelr)){install.packages("modelr"); library(modelr)} 
# load Google Maps
if(!require(ggmap)){install.packages("ggmap"); library(ggmap)}

# some dependencies for caret that aren't automatically installed
if(!require(ModelMetrics)){install.packages("ModelMetrics"); require(ModelMetrics)}
if(!require(recipes)){install.packages("recipes"); require(recipes)}
if(!require(DEoptimR)){install.packages("DEoptimR"); require(DEoptimR)}

if(!require(caret)){install.packages("caret"); require(caret)} # ML package WITHOUT its dependencies. Should not take as long
if(!require(dplyr)){install.packages("dplyr"); require(dplyr)}
if(!require(jsonlite)){install.packages("jsonlite"); require(jsonlite)}
set.seed(370)
```

## Reading and Organization Yelp Restaurants Dataset
```{r, echo=FALSE, include=FALSE}
# Getting and filtering data
#json_file <- "SeattleYelpRestaurantsWithCategory.json"
#json_file <- "SeattleYelpRestaurantsWithHealth.json"
json_file <- "SeattleYelpRestaurantsWithNewHealthInfo.json"
yelp.data <- fromJSON(json_file) %>% filter(!is.na(price)) %>% filter(!is.na(reviewCount)) %>% filter(!is.na(censusMedianHHIncome)) %>% filter(!is.na(censusIncomePerCapita)) %>% filter(!is.na(censusGiniIndexOfInequality)) %>% filter(!is.na(rating)) %>% filter(!is.na(category)) %>% filter(!is.na(censusTract)) %>% filter(!is.na(recentHealthInspectionScore)) %>% filter(!is.na(restaurantMaxSeats)) %>% filter(!is.na(totalInspectionScore))

# convert price into numeric value
price.num <- c(nchar(yelp.data[,"price"]))
yelp.data$price = price.num

# cast Health Inspection categorical data into factor for dummy variables
yelp.data$healthInspectionResult <- as.factor(yelp.data$recentHealthInspectionResult)
yelp.data$healthViolationType <- as.factor(yelp.data$recentHealthViolationType)
yelp.data$recentHealthInspectionGrade <- as.factor(yelp.data$recentHealthInspectionGrade)

# create dummy variables for healthInspectionResult, inspection grade and healthViolationType 
dmy <- dummyVars(~ recentHealthInspectionResult + recentHealthViolationType + recentHealthInspectionGrade, data=yelp.data) # notice how we specify which columns to "dumbify""

#dmy <- dummyVars(~ ., data=yelp.data) # or run this to dumbify all variables that are factors

# add dummy variables data frame to the rest
yelp.data <- data.frame(yelp.data,predict(dmy, newdata = yelp.data))

# define the low/high quantile threshold by MedianHHincome
low <- quantile(yelp.data$censusMedianHHIncome, 0.33) # cutoff number
high <- quantile(yelp.data$censusMedianHHIncome, 0.66) # cutoff number

# filter data into 2 distinct set: high SES and low SES
above.avg <- yelp.data %>% filter(censusMedianHHIncome > high)
below.avg <- yelp.data %>% filter(censusMedianHHIncome < low)
mid.avg <- yelp.data %>% filter(censusMedianHHIncome < high & censusMedianHHIncome > low)

# filter data to only have relating variables: reviewCount, price and rating
# this dataset is used for modeling

# only using healthInspectionResult dummy variables: complete, incomplete, Satisfactory and unsatisfactory
above.avg.wo.census <- subset(above.avg, select=c("rating", "reviewCount", "recentHealthInspectionScore","totalInspectionScore", "totalInspectionCount", "avgInspectionScore", "restaurantTotalMonths", "restaurantMaxSeats", "recentHealthInspectionResultSatisfactory", "recentHealthInspectionResultUnsatisfactory", "recentHealthViolationType.1", "recentHealthViolationTypeblue", "recentHealthViolationTypered", "recentHealthInspectionGrade.1", "recentHealthInspectionGrade.2", "recentHealthInspectionGrade.3", "recentHealthInspectionGrade.4"))
below.avg.wo.census <- subset(below.avg, select=c("rating", "reviewCount", "recentHealthInspectionScore","totalInspectionScore", "totalInspectionCount", "avgInspectionScore", "restaurantTotalMonths", "restaurantMaxSeats", "recentHealthInspectionResultSatisfactory", "recentHealthInspectionResultUnsatisfactory", "recentHealthViolationType.1", "recentHealthViolationTypeblue", "recentHealthViolationTypered", "recentHealthInspectionGrade.1", "recentHealthInspectionGrade.2", "recentHealthInspectionGrade.3", "recentHealthInspectionGrade.4"))

check.unique.dummy.variable <- function(dataset) {
  result.complete <-  length(unique(dataset$recentHealthInspectionResultComplete)) > 1
  result.incomplete <- length(unique(dataset$recentHealthInspectionResultIncomplete)) > 1
  result.not.ready <- length(unique(dataset$recentHealthInspectionResultNot.Ready.For.Inspection)) > 1
  result.satisfactory <- length(unique(dataset$recentHealthInspectionResultSatisfactory)) > 1
  result.unsatisfactory <- length(unique(dataset$recentHealthInspectionResultUnsatisfactory)) > 1
  result.violationtype <- length(unique(dataset$recentHealthViolationType)) > 1
  result.violationBlue <- length(unique(dataset$recentHealthViolationTypeblue)) > 1
  result.violationRed <- length(unique(dataset$recentHealthViolationTypered)) > 1
  result.inspectionGrade1 <- length(unique(dataset$recentHealthInspectionGrade.1)) > 1
  result.inspectionGrade2 <- length(unique(dataset$recentHealthInspectionGrade.2)) > 1
  result.inspectionGrade3 <- length(unique(dataset$recentHealthInspectionGrade.3)) > 1
  result.inspectionGrade4 <- length(unique(dataset$recentHealthInspectionGrade.4)) > 1
  return(c(result.complete,result.incomplete,result.not.ready,result.satisfactory,result.unsatisfactory,result.violationtype,result.violationBlue,result.violationRed,result.inspectionGrade1,result.inspectionGrade2,result.inspectionGrade3,result.inspectionGrade4))
}

# yelp restaurants with all variables/feature
filtered.yelp.w.census <- subset(yelp.data, select=c("reviewCount", "censusTract", "censusMedianHHIncome", "censusIncomePerCapita", "censusGiniIndexOfInequality", "category", "price"))
```

## Plotting all Yelp Restaurants in Maps
```{r, echo=FALSE}
mapgilbert <- get_map(location = c(lon = mean(yelp.data$longitude), lat = mean(yelp.data$latitude)), zoom = 11, maptype = "roadmap", scale = "auto")

# plotting the map with some points on it
ggmap(mapgilbert) +
  geom_point(data = yelp.data, aes(x = yelp.data$longitude, y = yelp.data$latitude, fill = "red", alpha = 0.8), size = 2, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE)

```
The map above is just plotting all the restaurants. In the future we may give different color for each rating. You guys can just talk about the general information about the data set. Some info listed below

```{r}
# total number of restaurants we're observing; we filter out restaurants that don't have PriceLevel, Category, and Rating
restaurantCount <- nrow(yelp.data)
restaurantCount

# all category types
allCategory <- as.data.frame(table(yelp.data$category))
allCategory

#total category
categoryCount <- nrow(allCategory)
categoryCount
```



## Distribution of Yelp-associated Restaurants in Seattle
```{r, echo=FALSE}
sorted.highSES <- transform(above.avg, censusTract = reorder(censusTract, censusMedianHHIncome))

highSES <- ggplot(sorted.highSES, aes(censusTract, fill=censusMedianHHIncome)) +geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(title="Seattle Restaurants in High Socio-Economic Neighborhoods on Yelp", y="Number of Restaurants", x="Seattle Census Tract Code", fill="Median Household Income ($)", caption="(based on data from Yelp Fusion API and Seattle, FCC Data, Census Reporter, and the U.S. Census Bureau)")

sorted.midSES <- transform(mid.avg, censusTract = reorder(censusTract, censusMedianHHIncome))

midSES <- ggplot(sorted.midSES, aes(censusTract, fill=censusMedianHHIncome)) +geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(title="Seattle Restaurants in Median Socio-Economic Neighborhoods on Yelp", y="Number of Restaurants", x="Seattle Census Tract Code", fill="Median Household Income ($)", caption="(based on data from Yelp Fusion API and Seattle, FCC Data, Census Reporter, and the U.S. Census Bureau)")

sorted.lowSES <- transform(below.avg, censusTract = reorder(censusTract, censusMedianHHIncome))
lowSES <- ggplot(sorted.lowSES, aes(censusTract, fill=censusMedianHHIncome)) +geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(title="Seattle Restaurants in Low Socio-Economic Neighborhoods on Yelp", y="Number of Restaurants", x="Seattle Census Tract Code", fill="Median Household Income ($)", caption="(based on data from Yelp Fusion API and Seattle, FCC Data, Census Reporter, and the U.S. Census Bureau)")

```

We broke down data to high, mid, and low SES based on median household income, but not Seattle household income. we split dataset in 3. lower third consider low SES, mid third consider mid SES.

High SES : anything more than 74,559
Low SES: anything less than 59,275
Mid SES: anything in between

```{r}
highSES
```

This graph is plotting all the restaurants that are consider high SES by census tract code. 
X axis: The census tract code is sorted from left to right by median household income(low to high)
Y axis: the sum of restaurants in the specific census tract

```{r}
midSES
```

This graph is plotting all the restaurants that are consider mid SES by census tract code. 
X axis: The census tract code is sorted from left to right by median household income(low to high)
Y axis: the sum of restaurants in the specific census tract

```{r}
lowSES
```

This graph is plotting all the restaurants that are consider low SES by census tract code. 
X axis: The census tract code is sorted from left to right by median household income(low to high)
Y axis: the sum of restaurants in the specific census tract


## Distribution of Yelp-associated Restaurants By Price level in Seattle
```{r, echo=FALSE}
rating.data <- yelp.data %>% mutate(MedianHHLevel = ifelse(censusMedianHHIncome < low, "Low SES", 
                                                    ifelse(censusMedianHHIncome > high, "High SES", "Mid SES")))

priceFreq <- as.data.frame(table(rating.data$price,rating.data$MedianHHLevel))

priceFreq$percentage = priceFreq$Freq / sum(priceFreq$Freq)
  
restaurantsByPrice <- ggplot(priceFreq, aes(fill=Var2, y=Freq, x=Var1)) + 
    geom_bar( stat="identity", position="dodge") + labs(title="Distribution of Seattle Restaurants by Price Level on Yelp", y="Number of Restaurants", x="Price Level on Yelp ($ - $$$$)", fill="Socio-Economic Level \n(based on Median Household Income)", caption="(based on data from Yelp Fusion API and Seattle, Census Reporter, and the U.S. Census Bureau)")
restaurantsByPrice
```

This graph is plotting the distribution of restaurants grouped by High/low/mid SES. 
X axis: Price level or $ sign (low= 1 sign to high= 4 signs)
Y axis: the sum of restaurants in the price level


## Distribution of Yelp-associated Restaurants by Rating Seattle
```{r, echo=FALSE}
ratingFreq <- as.data.frame(table(yelp.data$rating))

ratingFreq$percentage = ratingFreq$Freq / sum(ratingFreq$Freq)
  
restaurantsByRating <- ggplot(ratingFreq, aes(y=percentage, x=Var1,group=1)) + 
    geom_line() + labs(title="Distribution of Seattle Restaurants by Rating on Yelp", y="Total Proportion of Restaurants", x="Average Rating on Yelp", caption="(based on data from Yelp Fusion API)")
restaurantsByRating
```

This graph is plotting the distribution of restaurants grouped by rating. 
X axis: rating (low to high)
Y axis: percentage (frquency / sum of all restaurants) in number of restaurants for each rating


## Distribution of Yelp-associated Restaurants Rating vs Household Income Violin Plot
```{r, echo=FALSE}
ratingViolinPlot <- ggplot(yelp.data, aes(as.factor(rating), censusMedianHHIncome)) +
  geom_violin() +
  geom_boxplot(width=0.1) +
  stat_summary(fun.y=mean, geom="point", shape=23, size=2, color="blue") +
  stat_summary(fun.y=median, geom="point", size=2, color="red") +
  labs(title="Distribution of Seattle Restaurants Rating \nin various Median Household Income on Yelp", y="Median Household Income ($)", x="Average Rating on Yelp", caption="(based on data from Yelp Fusion API and Seattle, Census Reporter, and the U.S. Census Bureau)")

ratingViolinPlot
```

Eugenia might have a better idea for this graph.

## Features Ranked by Importance for high SES
```{r, echo=FALSE}
control <- trainControl(method="repeatedcv", number = 10, repeats = 3)

highSES_model <- train(rating ~., data=above.avg.wo.census, method = "knn", preProcess = "scale", trControl = control)

highSES_importance <- varImp(highSES_model)

ggplot(highSES_importance)
```

## Features Ranked by Importance for high SES
```{r, echo=FALSE}
control <- trainControl(method="repeatedcv", number = 10, repeats = 3)

lowSES_model <- train(rating ~., data=below.avg.wo.census, method = "knn", preProcess = "scale", trControl = control)

lowSES_importance <- varImp(lowSES_model)

ggplot(lowSES_importance)
```

## Create proportion for high and low SES resturants
```{r, echo=FALSE, include=FALSE}
# splitting boston data into train+validate and test sets

split_proportion = 0.8

# select outcome variable for below avg HH income
below.avg.outcome <- below.avg.wo.census %>% dplyr::select(rating) # rating column only

# randomly select indices for train/validate set
below.avg.train_ind <- createDataPartition(below.avg.outcome$rating, p = split_proportion, list = FALSE)
below.avg.train <- below.avg.wo.census[below.avg.train_ind,] # get training below avg data
below.avg.test <- below.avg.wo.census[-below.avg.train_ind,] # get test below avg data

# select outcome variable for above avg HH income
above.avg.outcome <- above.avg.wo.census %>% dplyr::select(rating) # rating column only

# randomly select indices for train/validate set
above.avg.train_ind <- createDataPartition(above.avg.outcome$rating, p = split_proportion, list = FALSE)
above.avg.train <- above.avg.wo.census[above.avg.train_ind,] # get training above avg data
above.avg.test <- above.avg.wo.census[-above.avg.train_ind,] # get test above avg data

#yelp_test_x <- filtered.yelp %>% dplyr::select(-rating) # select predictor data for test set
#yelp_test_y <- filtered.yelp %>% dplyr::select(rating) # select outcome data for test set
```
## Creating training control
```{r, echo=FALSE, include=FALSE}
ctrl <- trainControl(method = "repeatedcv", number=10, repeats=3) # 10 fold cross-validation, repeated 3 times. better way to do it but takes longer.
```

## SVM Regression Model for High SES Restaurants
```{r}
# high SES SVM modeling
model_svm_high <- train(rating ~ .,
                  data = above.avg.train,
                  method = "svmRadial",
                  trControl=ctrl,   # Radial kernel
                  tuneLength = 10)

predict_yelp_svm_high <- predict(model_svm_high, below.avg.test)
#postResample(predict_yelp_svm_high, below.avg$rating)

#actual minus predicted
delta.high.SES <- below.avg.test$rating - predict_yelp_svm_high
count.high.SES.below.line <- length(which(delta.high.SES < 0)) / length(delta.high.SES)
count.high.SES.below.line

count.high.SES.above.line <- length(which(delta.high.SES > 0)) / length(delta.high.SES)
count.high.SES.above.line

count.high.SES.on.line <- length(which(delta.high.SES == 0)) / length(delta.high.SES)
count.high.SES.on.line

highSES_with_residual <- below.avg.test %>% 
 mutate(
     residuals = rating - predict_yelp_svm_high
 )

ggplot(highSES_with_residual, aes(1:nrow(highSES_with_residual))) +
  geom_point(aes(y=residuals)) +
  geom_abline(intercept=0, slope = 0) + ylim(-3,3)
```


## SVM Regression Model for low SES Restaurants
```{r}
# low SES SVM modeling
model_svm_low <- train(rating ~ .,
                  data = below.avg.train,
                  method = "svmRadial",
                  trControl=ctrl,   # Radial kernel
                  tuneLength = 10)

predict_yelp_svm_low <- predict(model_svm_low, above.avg.test)
#postResample(predict_yelp_svm_low, below.avg$rating)

#actual minus predicted
delta.low.SES <- above.avg.test$rating - predict_yelp_svm_low
count.low.SES.below.line <- length(which(delta.low.SES < 0)) / length(delta.low.SES)
count.low.SES.below.line

count.low.SES.above.line <- length(which(delta.low.SES > 0)) / length(delta.low.SES)
count.low.SES.above.line

count.low.SES.on.line <- length(which(delta.low.SES == 0)) / length(delta.low.SES)
count.low.SES.on.line

lowSES_with_residual <- above.avg.test %>% 
 mutate(
     residuals = rating - predict_yelp_svm_low
 )

ggplot(lowSES_with_residual, aes(1:nrow(lowSES_with_residual))) +
  geom_point(aes(y=residuals)) +
  geom_abline(intercept=0, slope = 0) + ylim(-3,3)
```


## Organize Data for Subway

```{r}
# filter only Subways for high and low SES
above.avg.subway <- filter(above.avg, name %in% "Subway")
below.avg.subway <- filter(below.avg, name %in% "Subway")

# remove violationTypeRed, InspectionGrade 2 ,3 ,4 for above.avg since those columns aren't unique
above.avg.subway <- subset(above.avg.subway, select=c("rating", "reviewCount", "recentHealthInspectionScore","totalInspectionScore", "totalInspectionCount", "avgInspectionScore", "restaurantTotalMonths", "restaurantMaxSeats", "recentHealthInspectionResultSatisfactory", "recentHealthInspectionResultUnsatisfactory", "recentHealthViolationType.1", "recentHealthViolationTypeblue", "recentHealthInspectionGrade.1"))

# remove InspectionGrade 3 ,4 for above.avg since those columns aren't unique
below.avg.subway <- subset(below.avg.subway, select=c("rating", "reviewCount", "recentHealthInspectionScore","totalInspectionScore", "totalInspectionCount", "avgInspectionScore", "restaurantTotalMonths", "restaurantMaxSeats", "recentHealthInspectionResultSatisfactory", "recentHealthInspectionResultUnsatisfactory", "recentHealthViolationType.1", "recentHealthViolationTypeblue",  "recentHealthInspectionGrade.1"))

subway.above.ctrl <- trainControl(method = "repeatedcv", number=nrow(above.avg.subway) - 1, repeats=3) # 10 fold cross-validation, repeated 3 times. better way to do it but takes longer.
subway.below.ctrl <- trainControl(method = "repeatedcv", number=nrow(below.avg.subway) - 1, repeats=3)
```

## Create proportion for high and low SES Subway
```{r, echo=FALSE, include=FALSE}
# splitting boston data into train+validate and test sets

split_proportion = 0.8

# select outcome variable for below avg HH income
below.avg.subway.outcome <- subway.below.avg.wo.census %>% dplyr::select(rating) # rating column only

# randomly select indices for train/validate set
below.avg.subway.train_ind <- createDataPartition(below.avg.subway.outcome$rating, p = split_proportion, list = FALSE)
below.avg.subway.train <- subway.below.avg.wo.census[below.avg.subway.train_ind,] # get training below avg data
below.avg.subway.test <- subway.below.avg.wo.census[-below.avg.subway.train_ind,] # get test below avg data

# select outcome variable for above avg HH income
above.avg.subway.outcome <- subway.above.avg.wo.census %>% dplyr::select(rating) # rating column only

# randomly select indices for train/validate set
above.avg.subway.train_ind <- createDataPartition(above.avg.subway.outcome$rating, p = split_proportion, list = FALSE)
above.avg.subway.train <- subway.above.avg.wo.census[above.avg.subway.train_ind,] # get training above avg data
above.avg.subway.test <- subway.above.avg.wo.census[-above.avg.subway.train_ind,] # get test above avg data
```

## Linear Model for High SES Subway
```{r}
# high SES SVM modeling
subway_lm_high <- train(rating ~ .,
                  data = above.avg.subway,
                  method = "lm",
                  trControl=subway.above.ctrl)

#subway_lm_high_2 <- lm(formula = rating ~ reviewCount * price, data = above.avg.subway.train)
summary(subway_lm_high)

predict_subway_lm_high <- predict(subway_lm_high, below.avg.subway)
#postResample(predict_yelp_svm_high, below.avg$rating)

#actual minus predicted
delta.subway.high.SES <- below.avg.subway$rating - predict_subway_lm_high
count.subway.high.SES.below.line <- length(which(delta.subway.high.SES < 0)) / length(delta.subway.high.SES)
count.subway.high.SES.below.line

count.subway.high.SES.above.line <- length(which(delta.subway.high.SES > 0)) / length(delta.subway.high.SES)
count.subway.high.SES.above.line

count.subway.high.SES.on.line <- length(which(delta.subway.high.SES == 0)) / length(delta.subway.high.SES)
count.subway.high.SES.on.line

subway_high_with_residual <- below.avg.subway %>% 
 mutate(
     residuals = rating - predict_subway_lm_high
 )

ggplot(subway_high_with_residual, aes(rating)) +
  geom_point(aes(y=residuals)) +
  geom_abline(intercept=0, slope = 0) + ylim(-2,2)
```

## Linear Model for low SES Subway
```{r}
# high SES SVM modeling
subway_lm_low <- train(rating ~ .,
                  data = below.avg.subway,
                  method = "lm",
                  trControl=subway.below.ctrl)

predict_subway_lm_low <- predict(subway_lm_low, above.avg.subway)
#postResample(predict_yelp_svm_high, below.avg$rating)

#actual minus predicted
delta.subway.low.SES <- above.avg.subway$rating - predict_subway_lm_low
count.subway.low.SES.below.line <- length(which(delta.subway.low.SES < 0)) / length(delta.subway.low.SES)
count.subway.low.SES.below.line

count.subway.low.SES.above.line <- length(which(delta.subway.low.SES > 0)) / length(delta.subway.low.SES)
count.subway.low.SES.above.line

count.subway.low.SES.on.line <- length(which(delta.subway.low.SES == 0)) / length(delta.subway.low.SES)
count.subway.low.SES.on.line

subway_low_with_residual <- above.avg.subway %>% 
 mutate(
     residuals = rating - predict_subway_lm_low
 )

ggplot(subway_low_with_residual, aes(rating)) +
  geom_point(aes(y=residuals)) +
  geom_abline(intercept=0, slope = 0) + ylim(-2,2)
```